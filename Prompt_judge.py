# -*- coding: utf-8 -*-
"""[CSC420M] Machine_Project_Prompt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10dm6yVB0irCas7pVr1fYUUoi0RP1RYeA

#LLM Translation Judge for English-to-Filipino Translations via Prompt Engineering

## Installation of Dependencies
"""

from google.colab import ai
import pandas as pd
import urllib.request
import numpy as np
from scipy.stats import spearmanr
import time
import random
import re

"""## Importing, Pre-processing, and Cleaning the dataset

### Importing the dataset

The dataset was downloaded from google sheets and uploaded into a public repository in github. From there the dataset is loaded into the notebook using urllib
"""

url = 'https://raw.githubusercontent.com/CarandangR/CSC420M-LLM-Translation-Judge-for-English-to-Filipino-Translations/main/Datasets%20-%20Training.csv'
filename = 'training_dataset.csv'

urllib.request.urlretrieve(url, filename)

df = pd.read_csv(filename)

df.head()

"""### Displaying the info of the dataset"""

df.info()

"""## Dataset Cleaning

### Dropping Rows with missing `English` or `Filipino-Correct/Filipino/Flawed` Texts
"""

df_cleaned = df.dropna(subset=['English', 'Filipino-Correct', 'Filipino-Flawed']).copy()

"""### Renaming of columns

This is for the columns to be easily handled by the LLM. This is how each column was renamed:
*   `English` was turned into `source_text`
*   `Filipino-Correct` was turned into `reference_translation`
*   `Filipino-Flawed` was turned into `translated_text`

"""

df_cleaned.rename(columns={
    'English': 'source_text',
    'Filipino-Correct': 'reference_translation',
    'Filipino-Flawed': 'translated_text',
}, inplace=True)

"""### Checking of duplicate strings"""

df_cleaned = df_cleaned[
    (df_cleaned['source_text'].str.strip() != '') &
    (df_cleaned['translated_text'].str.strip() != '')
].drop_duplicates(subset=['source_text', 'translated_text'])

"""### Displaying Cleaned dataset"""

df_cleaned.sample(5)

"""## Declaring the Prompting template

This template was made to take into consideration the different requirements specified in the machine project specifications. Things like Accuracy, Fluency, Coherence, Cultural Appropriateness, and Completeness are criterias that the LLM will evaluate on.
"""

prompt = """
You are a strict translation judge evaluating an English-to-Filipino translation. Your task is to Evaluate and score the translated text.

### Task Overview:
Generate an evaluation from the english source text, the translated text, and the reference translation.
English source text: {source_text}
Translated text: {translated_text}
Reference text: {reference_translation}
Note that the reference translation is the correct translation of the source text so it will get the perfect score. Keep in mind when evaluating

### Evaluation Criteria:
Each criteria below will be either 0 or 1 point each. The perfect score of the translation is 6 points and the lowest score is 0 points.
The criteria can only have a score of 1 if they meed the requirements of the criteria. otherwise it will always be 0.
1. Accuracy – Deduct if any part of the meaning differs from the reference or omits details.
2. Fluency – Deduct if grammar, style, or idiomatic expression is less natural than the reference.
3. Coherence – Deduct if the logical flow or structure differs from the reference without valid reason.
4. Cultural Appropriateness – Deduct if cultural tone, idioms, or respectful forms differ in a way that harms the message.
5. Guideline Adherence – Deduct if terminology/style differs from the reference in a way that breaks domain rules.
6. Completeness – Deduct if any information present in the reference is missing or altered.

### Sample Output
English - The actress has not given any further details about it.
Translated - Ang aktres ay hindi nagbigay ng anumang karagdagang mga detalye tungkol dito.
Accuracy: 1 point (The translated text accurately conveys the core meaning of the source text, which is that the actress has provided no new information.)
Fluency: 0 point (The use of the "Ang... ay..." construction is grammatically correct but less natural and fluent than the reference text's "Wala pang..." structure, which is more common in Filipino.)
Coherence: 0 point (The sentence structure, while logical, differs from the more integrated and seamless flow of the reference text. The reference's structure connects the ideas more smoothly.)
Cultural Appropriateness: 1 point (The tone of the translation is neutral and fact-based, which is culturally appropriate for the context. No inappropriate language is used.)
Guideline Adherence: 0 point (The translation deviates from the style of the reference text by using the formal "ay" construction and the literal "anumang" instead of the more idiomatic "Wala pang..." structure.)
Completeness: 1 point (The translated text includes all key pieces of information from the source sentence: the actress, the action of not giving, the further details, and the subject matter.)
Total Score: 3 points
"""

"""## Import and setup of the Large Language Model

### Import of the Large Language Model through the use of the API key
"""

ai.list_models()

"""### Selecting Random Prompts to test different types of Promts"""

random_idx = random.choice(df_cleaned.index)
random_row = df_cleaned.loc[random_idx]

source_text = random_row['source_text']
translated_text = random_row['translated_text']
reference_translation = random_row.get('reference_translation')

formatted_prompt = prompt.format(
    source_text=source_text,
    translated_text=translated_text,
    reference_translation=reference_translation
)

"""###"""

response = ai.generate_text(formatted_prompt, model_name='google/gemini-2.5-pro')

"""###Consistency Testing

Note: Due to the limited quota we had this error appeared during testing. This is also where Consistency was tested. Where we repeated the following prompt for several times:

`prompt` = '''
You are a strict translation judge evaluating an English-to-Filipino translation. Your task is to Evaluate and score the translated text.

### Task Overview:
Generate an evaluation from the english source text, the translated text, and the reference translation.
English source text: A race condition happens when threads access shared data unpredictably.
Translated text: Race condition ay kapag may naunang thread.
Reference text: Nangyayari ang race condition kapag hindi kontrolado ang pag-access ng mga thread sa iisang data.
Note that the reference translation is the correct translation of the source text so it will get the perfect score. Keep in mind when evaluating

### Evaluation Criteria:
Each criteria below will be either 0 or 1 point each. The perfect score of the translation is 6 points and the lowest score is 0 points.
The criteria can only have a score of 1 if they meed the requirements of the criteria. otherwise it will always be 0.
1. Accuracy – Deduct if any part of the meaning differs from the reference or omits details.
2. Fluency – Deduct if grammar, style, or idiomatic expression is less natural than the reference.
3. Coherence – Deduct if the logical flow or structure differs from the reference without valid reason.
4. Cultural Appropriateness – Deduct if cultural tone, idioms, or respectful forms differ in a way that harms the message.
5. Guideline Adherence – Deduct if terminology/style differs from the reference in a way that breaks domain rules.
6. Completeness – Deduct if any information present in the reference is missing or altered.

### Sample Output
English - The actress has not given any further details about it.
Translated - Ang aktres ay hindi nagbigay ng anumang karagdagang mga detalye tungkol dito.
Accuracy: 1 point (The translated text accurately conveys the core meaning of the source text, which is that the actress has provided no new information.)
Fluency: 0 point (The use of the "Ang... ay..." construction is grammatically correct but less natural and fluent than the reference text's "Wala pang..." structure, which is more common in Filipino.)
Coherence: 0 point (The sentence structure, while logical, differs from the more integrated and seamless flow of the reference text. The reference's structure connects the ideas more smoothly.)
Cultural Appropriateness: 1 point (The tone of the translation is neutral and fact-based, which is culturally appropriate for the context. No inappropriate language is used.)
Guideline Adherence: 0 point (The translation deviates from the style of the reference text by using the formal "ay" construction and the literal "anumang" instead of the more idiomatic "Wala pang..." structure.)
Completeness: 1 point (The translated text includes all key pieces of information from the source sentence: the actress, the action of not giving, the further details, and the subject matter.)
Total Score: 3 points
'''

These results although was not shown due to them not being saved to a variable showed a consistency of 100% for all 5 runs with all runs being given a score of 1. This being under the criteria of cultural appropriateness.
"""

print("Raw Response for Example 1:")
print(response)

"""## Testing using Validation set

### Loading Validation set
"""

valurl = 'https://raw.githubusercontent.com/CarandangR/CSC420M-LLM-Translation-Judge-for-English-to-Filipino-Translations/main/Datasets%20-%20Human-Labeled%20Validation%20Set.csv'

valfilename = 'validation_dataset.csv'
urllib.request.urlretrieve(valurl, valfilename)

val_df = pd.read_csv(valfilename)

val_df.head()

"""### Validation Set Cleaning

Dropping rows with missign `Source Text (English)`, `Target Text (Filipino)`, and `Final Score (1 - lowest, 5 - highest)` columns
"""

val_df_cleaned = val_df.dropna(subset=['Source Text (English)', 'Target Text (Filipino)', 'Final_score']).copy()

"""Renaming of Columns

This is for the columns to be easily handled by the LLM. This is how each column was renamed:
*   `Source Text (English)` was turned into `source_text`
*   `Target Text (Filipino)` was turned into `target_text`
*   `Final Score (1 - lowest, 5 - highest)` was turned into `score`

"""

val_df_cleaned.rename(columns={
    'Source Text (English)': 'source_text',
    'Target Text (Filipino)': 'target_text',
    'Final_score': 'score',
}, inplace=True)

"""Checking of duplicate strings"""

val_df_cleaned = val_df_cleaned[
    (val_df_cleaned['source_text'].str.strip() != '') &
    (val_df_cleaned['target_text'].str.strip() != '')
].drop_duplicates(subset=['source_text', 'target_text'])

"""Checking Cleaned dataset"""

val_df_cleaned.sample(5)

"""Define prompt to use for validation"""

valprompt = """
You are a strict translation judge evaluating an English-to-Filipino translation. Your task is to Evaluate and score the translated text.

### Task Overview:
Generate an evaluation from the english source text, the translated text, and the reference translation.
English source text: {source_text}
Translated text: {target_text}

### Evaluation Criteria:
Each criteria below will be either 0 or 1 point each. The perfect score of the translation is 6 points and the lowest score is 0 points.
The criteria can only have a score of 1 if they meed the requirements of the criteria. otherwise it will always be 0.
1. Accuracy – Deduct if any part of the meaning differs from the reference or omits details.
2. Fluency – Deduct if grammar, style, or idiomatic expression is less natural than the reference.
3. Coherence – Deduct if the logical flow or structure differs from the reference without valid reason.
4. Cultural Appropriateness – Deduct if cultural tone, idioms, or respectful forms differ in a way that harms the message.
5. Guideline Adherence – Deduct if terminology/style differs from the reference in a way that breaks domain rules.
6. Completeness – Deduct if any information present in the reference is missing or altered.

### Sample Output
English - The actress has not given any further details about it.
Translated - Ang aktres ay hindi nagbigay ng anumang karagdagang mga detalye tungkol dito.
Accuracy: 1 point (The translated text accurately conveys the core meaning of the source text, which is that the actress has provided no new information.)
Fluency: 0 point (The use of the "Ang... ay..." construction is grammatically correct but less natural and fluent than the reference text's "Wala pang..." structure, which is more common in Filipino.)
Coherence: 0 point (The sentence structure, while logical, differs from the more integrated and seamless flow of the reference text. The reference's structure connects the ideas more smoothly.)
Cultural Appropriateness: 1 point (The tone of the translation is neutral and fact-based, which is culturally appropriate for the context. No inappropriate language is used.)
Guideline Adherence: 0 point (The translation deviates from the style of the reference text by using the formal "ay" construction and the literal "anumang" instead of the more idiomatic "Wala pang..." structure.)
Completeness: 1 point (The translated text includes all key pieces of information from the source sentence: the actress, the action of not giving, the further details, and the subject matter.)
Total Score: 3 points
"""

"""Add Columns to the dataframe for the LLM to fill in"""

val_df_cleaned['llm_evaluation'] = None
val_df_cleaned['llm_score'] = None

for idx, row in val_df_cleaned.iterrows():

    formatted_prompt = valprompt.format(source_text=row['source_text'], target_text=row['target_text'])
    response = ai.generate_text(formatted_prompt, model_name='google/gemini-2.5-pro')
    evaluation_text = response
    val_df_cleaned.at[idx, 'llm_evaluation'] = evaluation_text

    # Extract total score using regex
    match = re.search(r'Total Score: (\d+) points', evaluation_text)
    if match:
        val_df_cleaned.at[idx, 'llm_score'] = int(match.group(1))

val_df_cleaned.info()

"""Exporting to .csv file"""

val_df_cleaned.to_csv('validated_dataset.csv', index=False)

"""Due to the prompts not being similar accross all prompts and responses. There were some empty fields in the in the score column. To make up for that, we decided to export the .csv file to be edited by a 3rd party program and exported back for results and analysis. The scores will be from the outputs generated by the LLM. So it would match the score given by the LLM.

## Results and Analysis

This section will analyze the results of the Prompt Engineered LLM judge to the results of human tested validation set.
"""

valurl = 'https://raw.githubusercontent.com/CarandangR/CSC420M-LLM-Translation-Judge-for-English-to-Filipino-Translations/main/prompt_validated_dataset.csv'

valfilename = 'prompt_results.csv'
urllib.request.urlretrieve(valurl, valfilename)

prompt_results = pd.read_csv(valfilename)

prompt_results.info()

"""### Normalizing results of the LLM"""

def normalize_score(raw):
    if raw >= 5:
        return 5
    elif 3 <= raw <= 4:
        return 3
    else:
        return 1

prompt_results["llm_score_normalized"] = prompt_results["llm_score"].apply(normalize_score)

"""### Evaluation metrics"""

spearman_corr, p_value = spearmanr(prompt_results["score"], prompt_results["llm_score_normalized"])

variation = prompt_results.groupby("source_text")["llm_score_normalized"].std(ddof=0).fillna(0)
inconsistent_ratio = (variation > 0.1).mean() * 100

explainability_ratio = (prompt_results["llm_evaluation"].str.strip() != "").mean() * 100

coverage_metric = prompt_results["Contributor"].nunique()

# Display results
print(f"Spearman’s ρ: {spearman_corr:.3f} (p={p_value:.3g})")
print(f"Explainability ratio: {explainability_ratio:.2f}%")
print(f"Coverage (unique contributors): {coverage_metric}")

"""### Checking for Samples that has great effect in the Spearman Score"""

prompt_results["score_diff"] = (prompt_results["score"] - prompt_results["llm_score_normalized"]).abs()

largest_disagreements = prompt_results.sort_values(by="score_diff", ascending=False)

big_gaps = largest_disagreements[largest_disagreements["score_diff"] >= 2]

print(big_gaps[["source_text", "target_text", "score", "llm_score_normalized", "llm_evaluation"]])

big_gaps.to_csv('big_gaps.csv', index=False)

"""### Comparative metrics to comapre against Agentic System

These are more metrics that are used to provide more comparison
"""

def snap_to_coarse_label(x):
    # nearest among 1,3,5
    choices = np.array([1,3,5])
    nearest = choices[np.argmin(np.abs(choices - x))]
    label_map = {1: 'poor', 3: 'good', 5: 'excellent'}
    return nearest, label_map[int(nearest)]

prompt_results['human_label'] = prompt_results['score'].apply(lambda x: snap_to_coarse_label(x)[1])

# For LLM normalized_total we expect values in {1,3,5} already; map to text
label_map = {1: 'poor', 3: 'good', 5: 'excellent'}
prompt_results['llm_label'] = prompt_results['llm_score_normalized'].map(label_map)

label_agreement = (prompt_results['score'] == prompt_results['llm_score_normalized']).mean()
print(f"\nLabel agreement (human vs LLM): {label_agreement*100:.1f}% ({int((label_agreement)*len(prompt_results))} / {len(prompt_results)})")

prompt_results['abs_diff'] = (prompt_results['score'] - prompt_results['llm_score_normalized']).abs()
mean_abs_diff = prompt_results['abs_diff'].mean()
print(f"\nMean absolute difference (|human - llm|): {mean_abs_diff:.3f}")

"""## Output as JSON File"""

prompt_results.to_json("prompt_results.json", orient="records", force_ascii=False, indent=2)